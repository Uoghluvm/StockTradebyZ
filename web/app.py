import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import sys
import subprocess
import os
import json
import time
from datetime import datetime, date, timedelta

# å¼•å…¥è‡ªå®šä¹‰ Swiss Style æ ·å¼
from utils.ui import inject_swiss_style, swiss_header
# å¼•å…¥å¤šè¯­è¨€æ”¯æŒ
from utils.lang import get_text, TRANSLATIONS

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="StockTrade Swiss Lab",
    page_icon="ğŸ‡¨ğŸ‡­",
    layout="wide",
    initial_sidebar_state="expanded",
)

# æ³¨å…¥ CSS
inject_swiss_style()

# ---------- çŠ¶æ€ç®¡ç† ----------
if 'language' not in st.session_state:
    st.session_state['language'] = 'CN'

def T(key, **kwargs):
    return get_text(st.session_state['language'], key, **kwargs)

# ---------- å·¥å…·å‡½æ•° ----------

@st.cache_data
def load_summary(file_mtime: float = 0):
    """åŠ è½½ç­–ç•¥è¯„æµ‹æŠ¥å‘Š (å‚æ•° file_mtime ç”¨äºç¼“å­˜åˆ·æ–°)"""
    file_path = Path("results/ç­–ç•¥è¯„æµ‹æŠ¥å‘Š_æ±‡æ€».csv")
    if file_path.exists():
        return pd.read_csv(file_path)
    return pd.DataFrame()

def get_summary_with_refresh():
    """è·å–ç­–ç•¥æŠ¥å‘Šï¼Œè‡ªåŠ¨æ ¹æ®æ–‡ä»¶ä¿®æ”¹æ—¶é—´åˆ·æ–°ç¼“å­˜"""
    file_path = Path("results/ç­–ç•¥è¯„æµ‹æŠ¥å‘Š_æ±‡æ€».csv")
    mtime = file_path.stat().st_mtime if file_path.exists() else 0
    return load_summary(mtime)

@st.cache_data
def get_index_stats(code):
    """è·å–æŒ‡æ•°ç»Ÿè®¡ä¿¡æ¯ (Sharpe, WR, Score)"""
    try:
        p = Path(f"data_parquet/{code}.parquet")
        if p.exists():
            df = pd.read_parquet(p, columns=['date', 'close'])
            df['pct_change'] = df['close'].pct_change() * 100
            df = df.dropna()
            # ä»…å–æœ€è¿‘1å¹´
            min_d = df['date'].max() - timedelta(days=365)
            df = df[df['date'] >= min_d]
            
            if df.empty: return None
            
            ret_mean = df['pct_change'].mean()
            ret_std = df['pct_change'].std()
            win_rate = (df['pct_change'] > 0).sum() / len(df)
            
            sharpe = ret_mean / ret_std if ret_std > 0 else 0
            score = sharpe * win_rate
            
            return {
                "sharpe": sharpe,
                "win_rate": win_rate * 100,
                "score": score
            }
    except:
        return None

def get_logs_dates():
    """è·å– logs/ ç›®å½•ä¸‹æ‰€æœ‰çš„æ—¥å¿—æ—¥æœŸ"""
    files = list(Path("logs").glob("*é€‰è‚¡.csv"))
    dates = []
    for f in files:
        # 2026-01-20é€‰è‚¡.csv -> 2026-01-20
        d_str = f.stem.replace("é€‰è‚¡", "")
        try:
            dates.append(datetime.strptime(d_str, "%Y-%m-%d").date())
        except ValueError:
            pass
    dates.sort(reverse=True)
    return dates

def is_trading_day(date_str: str) -> bool:
    """æ£€æŸ¥æŸæ—¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ (ä¼˜å…ˆä½¿ç”¨ Parquetï¼Œå›é€€åˆ° CSV)"""
    parquet_dir = Path("data_parquet")
    data_dir = Path("data")
    
    # æŠ½æ ·æ£€æŸ¥å‡ åªå¤§ç›˜è‚¡çš„æ•°æ®
    sample_stocks = ["000001", "600000", "000002"]
    
    for code in sample_stocks:
        # ä¼˜å…ˆæ£€æŸ¥ Parquet
        parquet_path = parquet_dir / f"{code}.parquet"
        if parquet_path.exists():
            try:
                df = pd.read_parquet(parquet_path, columns=['date'])
                if date_str in df['date'].astype(str).values:
                    return True
            except Exception:
                pass
        
        # å›é€€åˆ° CSV
        csv_path = data_dir / f"{code}.csv"
        if csv_path.exists():
            try:
                df = pd.read_csv(csv_path, usecols=['date'])
                if date_str in df['date'].values:
                    return True
            except Exception:
                continue
    
    return False

def load_daily_result_by_date(d: date):
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„å›æµ‹ç»“æœ (ä¼˜å…ˆæ‰¾CSVï¼Œæ²¡æœ‰åˆ™å°è¯•å®æ—¶å›æµ‹)"""
    date_str = d.strftime("%Y-%m-%d")
    csv_path = Path(f"results/å›æµ‹ç»“æœ_{date_str}.csv")
    
    if csv_path.exists():
        return pd.read_csv(csv_path)
    
    # å¦‚æœ CSV ä¸å­˜åœ¨ä½†æœ‰ Logï¼Œè¯´æ˜å¯èƒ½è¿˜æ²¡å›æµ‹ï¼Œå°è¯•è‡ªåŠ¨å›æµ‹
    log_path = Path(f"logs/{date_str}é€‰è‚¡.csv")
    if log_path.exists():
        try:
            # è‡ªåŠ¨è§¦å‘å›æµ‹
            subprocess.run([sys.executable, "scripts/backtest.py", str(log_path)], check=True)
            if csv_path.exists():
                return pd.read_csv(csv_path)
        except Exception:
            pass
            
    return pd.DataFrame()

def get_activity_data():
    """è·å–æ¯æ—¥é€‰è‚¡æ•°é‡ç»Ÿè®¡ (ä» logs/ è¯»å–é€‰è‚¡ CSV)"""
    data = []
    logs_dir = Path("logs")
    if logs_dir.exists():
        for f in logs_dir.glob("*é€‰è‚¡.csv"):
            try:
                # 2026-01-20é€‰è‚¡.csv -> 2026-01-20
                date_str = f.stem.replace("é€‰è‚¡", "")
                
                # CSV æ–‡ä»¶è¡Œæ•° = è‚¡ç¥¨æ•° + 1 (è¡¨å¤´)
                with open(f, 'rb') as fp:
                    count = sum(1 for _ in fp) - 1
                if count < 0:
                    count = 0
                
                data.append({
                    'date': pd.to_datetime(date_str).date(),
                    'count': count
                })
            except Exception:
                pass
    
    if not data:
        return pd.DataFrame(columns=['date', 'count'])
        
    df = pd.DataFrame(data)
    df = df.sort_values('date')
    return df

def plot_activity_heatmap(df):
    """ç»˜åˆ¶ GitHub é£æ ¼çš„æ—¥å†çƒ­åŠ›å›¾ (High Fidelity Replica)"""
    if df.empty:
        return None
        
    # GitHub Color Palette (Light Mode)
    # 0: Gray, 1: Light Green, 2: Medium Green, 3: Dark Green, 4: Darkest Green
    colors = ['#ebedf0', '#9be9a8', '#40c463', '#30a14e', '#216e39']
    
    # Range configuration
    max_days = 365 # 1 year view
    end_date = date.today()
    start_date = end_date - timedelta(days=364) # Ensure full year relative to today
    
    # Align start_date to the previous Sunday (GitHub style columns start on Sunday)
    # But for Mon start logic:
    # Let's stick to Mon start for simplicity or adjust to Sun. GitHub is Sun-Sat. 
    # Let's use Mon-Sun (0-6) which is standard ISO.
    weekday_start = 0 # Monday
    
    start_date = start_date - timedelta(days=(start_date.weekday() - weekday_start) % 7)
    
    all_dates = pd.date_range(start_date, end_date, freq='D').date
    grid_df = pd.DataFrame({'date': all_dates})
    grid_df = grid_df.merge(df, on='date', how='left').fillna(0)
    
    # Binning counts into levels 0-4
    # Calculate quantiles for non-zero counts
    non_zero_counts = grid_df[grid_df['count'] > 0]['count']
    if not non_zero_counts.empty:
        q1 = non_zero_counts.quantile(0.25)
        q2 = non_zero_counts.quantile(0.50)
        q3 = non_zero_counts.quantile(0.75)
    else:
        q1, q2, q3 = 1, 2, 3 # Default if no data
        
    def get_level(c):
        if c == 0: return 0
        if c <= q1: return 1
        if c <= q2: return 2
        if c <= q3: return 3
        return 4
        
    grid_df['level'] = grid_df['count'].apply(get_level)
    grid_df['color'] = grid_df['level'].apply(lambda x: colors[x])
    
    # Calculate coordinates
    # X: Week number from start
    # Y: Day of week (0=Mon, 6=Sun) -> Invert for plot (0 at top)
    grid_df['week'] = grid_df.apply(lambda x: (x['date'] - start_date).days // 7, axis=1)
    grid_df['day'] = grid_df['date'].apply(lambda x: x.weekday())
    
    # Tooltip
    grid_df['text'] = grid_df.apply(lambda x: f"<b>{x['count']:.0f} stocks</b><br>{x['date'].strftime('%Y-%m-%d')}", axis=1)
    
    # Fixed size layout
    # Approx 53 weeks. Cell size ~12px + 3px gap.
    cell_size = 12
    gap = 2
    
    fig = go.Figure()
    
    fig.add_trace(go.Heatmap(
        z=grid_df['level'],
        x=grid_df['week'],
        y=grid_df['day'],
        colorscale=[
            [0.0, colors[0]], [0.2, colors[0]],
            [0.2, colors[1]], [0.4, colors[1]],
            [0.4, colors[2]], [0.6, colors[2]],
            [0.6, colors[3]], [0.8, colors[3]],
            [0.8, colors[4]], [1.0, colors[4]],
        ],
        showscale=False,
        xgap=gap,
        ygap=gap,
        hovertext=grid_df['text'],
        hoverinfo='text',
        zmin=0,
        zmax=4
    ))
    
    # Month Labels logic
    month_labels = []
    current_month = -1
    for d in all_dates:
        if d.weekday() == 0 and d.month != current_month: # First monday of new month roughly
            # Calculate simple approximate week x
            w = (d - start_date).days // 7
            month_labels.append(dict(
                x=w, y=-1, text=d.strftime('%b'), showarrow=False,
                xanchor='left', yanchor='bottom', font=dict(size=10, color='#767676')
            ))
            current_month = d.month
            
    fig.update_layout(
        height=160, # Compact height
        width=800,  # Fixed width to prevent stretching
        margin=dict(l=20, r=20, t=30, b=20),
        xaxis=dict(
            showticklabels=False, 
            showgrid=False, 
            zeroline=False, 
            fixedrange=True,
            range=[-0.5, 53]
        ),
        yaxis=dict(
            tickmode='array',
            ticktext=['Mon', '', 'Wed', '', 'Fri', '', ''],
            tickvals=[0, 1, 2, 3, 4, 5, 6],
            showgrid=False, 
            zeroline=False, 
            autorange="reversed",
            fixedrange=True,
            tickfont=dict(size=10, color='#767676')
        ),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        annotations=month_labels
    )
    
    return fig

def save_token(token):
    """ä¿å­˜ Token åˆ° .env"""
    env_path = Path(".env")
    lines = []
    if env_path.exists():
        with open(env_path, "r") as f:
            lines = f.readlines()
    
    # ç§»é™¤æ—§çš„ TUSHARE_TOKEN
    lines = [l for l in lines if not l.startswith("TUSHARE_TOKEN=")]
    lines.append(f"TUSHARE_TOKEN={token}\n")
    
    with open(env_path, "w") as f:
        f.writelines(lines)
    
    # ç«‹å³ç”Ÿæ•ˆ
    os.environ["TUSHARE_TOKEN"] = token

def run_process_with_progress(cmd, log_container=None, progress_bar=None, status_text=None):
    """è¿è¡Œå­è¿›ç¨‹å¹¶å®æ—¶è§£æè¿›åº¦"""
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
        universal_newlines=True
    )
    
    full_output = []
    
    for line in process.stdout:
        full_output.append(line)
        line_clean = line.strip()
        
        # è§£æè¿›åº¦ [LOAD] 100/5000 æˆ– [PROCESS] 50/5000
        if line_clean.startswith("[LOAD]") or line_clean.startswith("[PROCESS]"):
            try:
                parts = line_clean.split("]")[1].strip().split("/")
                current = int(parts[0])
                total = int(parts[1])
                
                if progress_bar and total > 0:
                    progress_bar.progress(min(current / total, 1.0))
                
                if status_text:
                    phase = "Loading Data" if "LOAD" in line_clean else "Processing Stocks"
                    status_text.text(f"{phase}... {current}/{total}")
            except Exception:
                pass
        
    process.wait()
    return "".join(full_output), process.returncode

# ---------- ä¾§è¾¹æ  ----------

# ç®€åŒ–çš„è¯­è¨€åˆ‡æ¢
if st.sidebar.button("ğŸ‡¨ğŸ‡³ / ğŸ‡ºğŸ‡¸", key="lang_toggle"):
    st.session_state['language'] = 'EN' if st.session_state['language'] == 'CN' else 'CN'
    st.rerun()

st.sidebar.markdown("### NAVIGATION")

page = st.sidebar.radio(
    "Go to", 
    ["DASHBOARD", "LABORATORY", "SIMULATION", "BACKTEST", "SETTINGS"], 
    format_func=lambda x: T(f'nav_{x.lower()}'),
    label_visibility="collapsed"
)

st.sidebar.markdown("---")
st.sidebar.markdown("**STATUS**")
token_set = os.environ.get("TUSHARE_TOKEN") or (
    "TUSHARE_TOKEN" in open(".env").read() if Path(".env").exists() else False
)

if token_set:
    st.sidebar.success(T('status_active'))
else:
    st.sidebar.error(T('status_missing'))

# ---------- é¡µé¢é€»è¾‘ ----------

if page == "DASHBOARD":
    swiss_header(T('dash_title'), T('dash_subtitle'))
    
    summary_df = get_summary_with_refresh()
    
    hs300 = get_index_stats('000300')
    zz1000 = get_index_stats('000852')
    
    if summary_df.empty:
        st.info(T('dash_no_data'))
    else:
        # 1. KPI
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(T('kpi_stocks'), f"{summary_df['æ€»èè‚¡æ•°'].sum()}")
        col2.metric(T('kpi_strategies'), f"{len(summary_df)}")
        col3.metric(T('kpi_score'), f"{summary_df.iloc[0]['ç»¼åˆå¾—åˆ†']:.1f}")
        col4.metric(T('kpi_days'), f"{len(get_logs_dates())}")
        
        st.markdown("---")
        
        # 0. å¸‚åœºæŒ‡æ•°èµ°åŠ¿
        st.markdown(f"##### {'å¸‚åœºå¤§ç›˜èµ°åŠ¿' if st.session_state['language'] == 'CN' else 'Market Index Trend'}")
        
        @st.cache_data
        def load_index_data():
            indices = {
                '000300': 'æ²ªæ·±300 (CSI300)',
                '000852': 'ä¸­è¯1000 (CSI1000)'
            }
            dfs = []
            for code, name in indices.items():
                p = Path(f"data_parquet/{code}.parquet")
                if p.exists():
                    try:
                        df = pd.read_parquet(p, columns=['date', 'close'])
                        df['date'] = pd.to_datetime(df['date'])
                        df['name'] = name
                        df = df.sort_values('date')
                        # ä»…ä¿ç•™æœ€è¿‘1å¹´
                        min_d = df['date'].max() - timedelta(days=365)
                        df = df[df['date'] >= min_d].copy()
                        
                        if not df.empty:
                            first_close = df.iloc[0]['close']
                            df['pct_change'] = (df['close'] - first_close) / first_close * 100
                            dfs.append(df)
                    except:
                        pass
            if dfs:
                return pd.concat(dfs, ignore_index=True)
            return pd.DataFrame()

        index_df = load_index_data()
        if not index_df.empty:
            fig_idx = px.line(index_df, x='date', y='pct_change', color='name', labels={'pct_change': 'æ¶¨è·Œå¹… (%)', 'date': 'æ—¥æœŸ'}, height=300)
            fig_idx.update_layout(
                margin=dict(l=0, r=0, t=30, b=0),
                legend=dict(orientation="h", y=1.05, yanchor="bottom", x=0, xanchor="left"),
                hovermode="x unified",
                title="",
                plot_bgcolor="white",
                paper_bgcolor="white"
            )
            fig_idx.update_xaxes(showgrid=True, gridcolor='#eee')
            fig_idx.update_yaxes(showgrid=True, gridcolor='#eee', zeroline=True, zerolinecolor='black')
            st.plotly_chart(fig_idx, use_container_width=True)
        else:
            st.caption("æš‚æ— æŒ‡æ•°æ•°æ®ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: `python scripts/fetch_kline.py --index`")

        st.markdown("---")
        
        # Activity Map
        st.markdown(f"##### {T('activity_map')}")
        st.caption(T('activity_help'))
        
        activity_df = get_activity_data()
        fig_map = plot_activity_heatmap(activity_df)
        if fig_map:
            st.plotly_chart(fig_map, use_container_width=True, config={'displayModeBar': False})
        
        st.markdown(f"### {T('chart_title')}")
        
        # 2. Chart
        try:
            fig = px.scatter(
                summary_df,
                x="æ”¶ç›˜_èƒœç‡%", y="æ”¶ç›˜_5æ—¥å‡%", size="æ€»èè‚¡æ•°", color="ç­–ç•¥",
                hover_name="ç­–ç•¥", hover_data=["æœ€ä½³å‘¨æœŸ", "æœ€ä½³å‡æ”¶"],
                height=500, color_discrete_sequence=px.colors.qualitative.Dark24
            )
            fig.update_layout(title="", plot_bgcolor="white", paper_bgcolor="white", font_family="Inter")
            
            # åæ ‡è½´ä¼˜åŒ–
            min_win = max(40, summary_df['æ”¶ç›˜_èƒœç‡%'].min() - 5)
            min_ret = min(0, summary_df['æ”¶ç›˜_5æ—¥å‡%'].min() - 1)
            max_ret = summary_df['æ”¶ç›˜_5æ—¥å‡%'].max() + 1
            
            fig.update_xaxes(range=[40, 105], title="èƒœç‡ (%)", showgrid=True, gridcolor='#eee', zeroline=True, zerolinecolor='black')
            fig.update_yaxes(range=[min_ret, max_ret] if min_ret < 0 else [0, max_ret], title="5æ—¥å‡æ”¶ç›Š (%)", showgrid=True, gridcolor='#eee', zeroline=True, zerolinecolor='black')
            
            fig.add_vline(x=50, line_dash="dash", line_color="gray", opacity=0.5)
            fig.add_hline(y=0, line_dash="solid", line_color="black", opacity=0.1)
            
            st.plotly_chart(fig, use_container_width=True)
        except ImportError:
            st.error(T('chart_missing_mpl'))
            
        # 3. Table
        st.markdown(f"### {T('table_title')}")
        column_config = {
            "ç­–ç•¥": st.column_config.TextColumn("ç­–ç•¥åç§°" if st.session_state['language'] == 'CN' else "Strategy", help="é€‰è‚¡ç­–ç•¥ç»„åˆåç§°"),
            "æ€»èè‚¡æ•°": st.column_config.NumberColumn("æ ·æœ¬æ•°" if st.session_state['language'] == 'CN' else "Samples", help="è¯¥ç­–ç•¥åœ¨å›æµ‹æœŸé—´æ€»å…±æ¨èçš„è‚¡ç¥¨æ•°é‡", format="%d"),
            "æ”¶ç›˜_5æ—¥å‡%": st.column_config.NumberColumn("5æ—¥æ”¶ç›Š%" if st.session_state['language'] == 'CN' else "5D Ret%", help="ä»¥æ”¶ç›˜ä»·ä¹°å…¥ï¼ŒæŒæœ‰5æ—¥åçš„å¹³å‡æ”¶ç›Šç‡", format="%.2f%%"),
            "å¼€ç›˜_5æ—¥å‡%": st.column_config.NumberColumn("5æ—¥æ”¶ç›Š%(å¼€)" if st.session_state['language'] == 'CN' else "5D Ret%(O)", help="ä»¥æ¬¡æ—¥å¼€ç›˜ä»·ä¹°å…¥ï¼ŒæŒæœ‰5æ—¥åçš„å¹³å‡æ”¶ç›Šç‡", format="%.2f%%"),
            "æ”¶ç›˜æ”¶ç›Š_1æ—¥(%)_mean": st.column_config.NumberColumn("1æ—¥%" if st.session_state['language'] == 'CN' else "1D%", format="%.2f%%"),
            "æ”¶ç›˜æ”¶ç›Š_2æ—¥(%)_mean": st.column_config.NumberColumn("2æ—¥%" if st.session_state['language'] == 'CN' else "2D%", format="%.2f%%"),
            "æ”¶ç›˜æ”¶ç›Š_3æ—¥(%)_mean": st.column_config.NumberColumn("3æ—¥%" if st.session_state['language'] == 'CN' else "3D%", format="%.2f%%"),
            "æ”¶ç›˜æ”¶ç›Š_5æ—¥(%)_mean": st.column_config.NumberColumn("5æ—¥%" if st.session_state['language'] == 'CN' else "5D%", format="%.2f%%"),
            "æ”¶ç›˜æ”¶ç›Š_10æ—¥(%)_mean": st.column_config.NumberColumn("10æ—¥%" if st.session_state['language'] == 'CN' else "10D%", format="%.2f%%"),
            "æœ€ä½³å‘¨æœŸ": st.column_config.TextColumn("æœ€ä½³æŒä»“" if st.session_state['language'] == 'CN' else "Best Hold", help="æ”¶ç›Šæœ€é«˜çš„æŒæœ‰å¤©æ•°"),
            "æœ€ä½³å‡æ”¶": st.column_config.NumberColumn("æœ€ä½³æ”¶ç›Š%" if st.session_state['language'] == 'CN' else "Best Ret%", format="%.2f%%"),
            "å‘¨æœŸè¯¦æƒ…": st.column_config.TextColumn("å„å‘¨æœŸæ”¶ç›Š" if st.session_state['language'] == 'CN' else "Period Details", width="large"),
            "æ”¶ç›˜_èƒœç‡%": st.column_config.NumberColumn("èƒœç‡%" if st.session_state['language'] == 'CN' else "Win Rate%", format="%.1f%%"),
            "æ”¶ç›Šæ ‡å‡†å·®": st.column_config.NumberColumn("æ³¢åŠ¨ç‡(Ïƒ)" if st.session_state['language'] == 'CN' else "Volatility(Ïƒ)", help="æ”¶ç›Šç‡çš„æ ‡å‡†å·®", format="%.2f"),
            "å¤æ™®æ¯”ç‡": st.column_config.NumberColumn("å¤æ™®æ¯”ç‡" if st.session_state['language'] == 'CN' else "Sharpe Ratio", help="å¹³å‡æ”¶ç›Š Ã· æ ‡å‡†å·®", format="%.2f"),
            "ç»¼åˆå¾—åˆ†": st.column_config.NumberColumn("ç»¼åˆè¯„åˆ†" if st.session_state['language'] == 'CN' else "Score", help="å¤æ™®æ¯”ç‡ Ã— èƒœç‡è°ƒæ•´å› å­", format="%.2f"),
        }
        st.dataframe(summary_df, use_container_width=True, column_config=column_config, hide_index=True)
        
        # 4. Distribution Chart
        st.markdown("---")
        st.markdown(f"### {'ç­–ç•¥æ”¶ç›Šåˆ†å¸ƒ' if st.session_state['language'] == 'CN' else 'Return Distribution'}")
        
        @st.cache_data
        def load_all_backtest_results():
            results_dir = Path("results")
            all_dfs = []
            for f in results_dir.glob("å›æµ‹ç»“æœ_*.csv"):
                try:
                    df = pd.read_csv(f)
                    all_dfs.append(df)
                except: pass
            return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()
        
        all_results = load_all_backtest_results()
        if not all_results.empty and 'ç­–ç•¥' in all_results.columns:
            strategies = all_results['ç­–ç•¥'].unique().tolist()
            selected_strategy = st.selectbox("é€‰æ‹©ç­–ç•¥" if st.session_state['language'] == 'CN' else "Select Strategy", strategies, index=0)
            
            if selected_strategy:
                strat_data = all_results[all_results['ç­–ç•¥'] == selected_strategy]
                ret_col = 'æ”¶ç›˜ä¹°å…¥æ”¶ç›Šç‡(%)'
                if ret_col in strat_data.columns:
                    returns = strat_data[ret_col].dropna()
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("æ ·æœ¬æ•°", f"{len(returns)}")
                    col2.metric("å‡å€¼", f"{returns.mean():.2f}%")
                    col3.metric("æ ‡å‡†å·®", f"{returns.std():.2f}")
                    col4.metric("å¤æ™®", f"{returns.mean() / returns.std():.2f}" if returns.std() > 0 else "N/A")
                    
                    fig = px.histogram(returns, nbins=30, title=f"{selected_strategy} - 5æ—¥æ”¶ç›Šåˆ†å¸ƒ", labels={'value': 'æ”¶ç›Šç‡(%)', 'count': 'é¢‘æ•°'}, color_discrete_sequence=['#4CAF50'])
                    fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="é›¶çº¿")
                    fig.add_vline(x=returns.mean(), line_dash="dot", line_color="blue", annotation_text=f"å‡å€¼:{returns.mean():.1f}%")
                    fig.update_layout(showlegend=False, height=350, plot_bgcolor="white", paper_bgcolor="white")
                    st.plotly_chart(fig, use_container_width=True)

        # 5. Scoring Formula
        st.markdown("---")
        with st.expander(T('score_formula_title'), expanded=False):
            st.markdown(T('score_formula_desc'))
            st.latex(r'''Score = Sharpe \times \frac{WinRate\%}{100} = \frac{AvgReturn}{StdDev} \times \frac{WinCount}{TotalCount}''')
            st.info("æ³¨ï¼šç”±äºå¤æ™®æ¯”ç‡æœ¬èº«æ•°å€¼è¾ƒå°ï¼ˆé€šå¸¸åœ¨0-3ä¹‹é—´ï¼‰ï¼Œç»¼åˆè¯„åˆ†é€šå¸¸å°äº1ã€‚è¯„åˆ†è¶Šé«˜è¶Šå¥½ã€‚")
            if hs300 and zz1000:
                st.markdown("**ä»æœ€è¿‘ä¸€å¹´èµ·çš„åŸºå‡†è¡¨ç° (Benchmark):**")
                b1, b2 = st.columns(2)
                b1.metric("æ²ªæ·±300 (CSI300)", f"Score: {hs300['score']:.4f}", f"Sharpe: {hs300['sharpe']:.2f} | WR: {hs300['win_rate']:.1f}%")
                b2.metric("ä¸­è¯1000 (CSI1000)", f"Score: {zz1000['score']:.4f}", f"Sharpe: {zz1000['sharpe']:.2f} | WR: {zz1000['win_rate']:.1f}%")

elif page == "LABORATORY":
    swiss_header(T('lab_title'), T('lab_subtitle'))
    
    tab1, tab2 = st.tabs([T('tab_daily'), T('tab_exec')])
    
    with tab1:
        st.markdown(f"##### {T('sel_date')}")
        available_dates = get_logs_dates()
        
        default_date = available_dates[0] if available_dates else date.today()
        
        # ä½¿ç”¨æ—¥å†æ§ä»¶
        selected_date = st.date_input("Calendar", value=default_date, label_visibility="collapsed")
        
        daily_df = load_daily_result_by_date(selected_date)
        
        if daily_df.empty:
            st.info(T('no_data_date', date=selected_date))
        else:
            # 1. è‡ªåŠ¨è¯†åˆ«ä»£ç åˆ—å¹¶è¡¥å…¨
            code_col = None
            if 'code' in daily_df.columns:
                code_col = 'code'
            elif 'symbol' in daily_df.columns:
                code_col = 'symbol'
            elif 'ts_code' in daily_df.columns:
                daily_df['code'] = daily_df['ts_code'].astype(str).str.split('.').str[0]
                code_col = 'code'
            
            if code_col:
                daily_df[code_col] = daily_df[code_col].astype(str).str.zfill(6)

            rec_count = len(daily_df)
            # å°è¯•è·å–å¹³å‡æ”¶ç›Šåˆ—
            ret_col = [c for c in daily_df.columns if 'æ”¶ç›˜æ”¶ç›Š' in c or 'æ”¶ç›˜ä¹°å…¥' in c][-1]
            avg_ret = daily_df[ret_col].mean() if not daily_df.empty else 0
            
            c1, c2 = st.columns(2)
            c1.metric(T('metric_selected'), f"{rec_count}")
            c2.metric(T('metric_avg_ret'), f"{avg_ret:.2f}%")
            
            st.markdown(f"##### {T('section_details')}")
            
            # å¼ºåˆ¶å°†ä»£ç åˆ—æ˜¾ç¤ºä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºæ•°å­—å»æ‰å‰å¯¼0
            # è‡ªåŠ¨è°ƒæ•´åˆ—å®½: use_container_width=True
            column_config = {}
            if code_col:
                column_config[code_col] = st.column_config.TextColumn("Code", width="medium")
                
            st.dataframe(
                daily_df, 
                use_container_width=True, 
                height=600,
                column_config=column_config
            )

    with tab2:
        st.markdown(f"##### {T('run_title')}")
        
        col_type, col_params = st.columns([1, 3])
        run_mode = col_type.radio("Mode", ["SINGLE DATE", "BATCH RANGE"], label_visibility="collapsed")
        
        if run_mode == "SINGLE DATE":
            exec_date = col_params.date_input(T('run_single_date'), value=date.today())
            
            if st.button(T('btn_run_single')):
                log_container = st.empty()
                try:
                    log_container.code(T('log_running', date=exec_date))
                    
                    # ä½¿ç”¨è¿›åº¦æ¡è¿è¡Œ
                    prog_bar = st.progress(0)
                    status_txt = st.empty()
                    
                    cmd_select = [sys.executable, "scripts/select_stock.py", "--date", str(exec_date)]
                    output, ret_code = run_process_with_progress(cmd_select, progress_bar=prog_bar, status_text=status_txt)
                    
                    if ret_code == 0:
                        prog_bar.empty()
                        status_txt.empty()
                        st.success(T('success_select'))
                        
                        log_path = f"logs/{exec_date}é€‰è‚¡.csv"
                        subprocess.run([sys.executable, "scripts/backtest.py", log_path], capture_output=True)
                        
                        st.success(T('success_finish'))
                        subprocess.run([sys.executable, "scripts/analyze_results.py"], check=False)
                    else:
                        st.error("Select failed")
                        log_container.code(output)
                except Exception as e:
                    st.error(f"{T('error_failed')} {e}")
                    
        else: # BATCH RANGE
            c1, c2 = col_params.columns(2)
            start_d = c1.date_input(T('run_start_date'), value=date.today() - timedelta(days=7))
            end_d = c2.date_input(T('run_end_date'), value=date.today())
            skip_exist = col_params.checkbox(T('run_skip_existing'), value=True)
            
            # å¹¶è¡Œåº¦æ§åˆ¶
            parallel_degree = col_params.slider(
                "âš¡ " + ("Parallel Degree" if st.session_state.get('lang') == 'en' else "å¹¶è¡Œåº¦"),
                min_value=1, max_value=6, value=2,
                help="Number of dates to process simultaneously. Higher = faster but more resource usage."
            )
            
            if st.button(T('btn_run_batch')):
                progress_bar = st.progress(0)
                status_text = st.empty()
                log_box = st.expander("Execution Log", expanded=True)
                
                # ä½¿ç”¨ batch_run.py è¿›è¡Œå¹¶è¡Œå¤„ç†
                start_str = str(start_d)
                end_str = str(end_d)
                
                log_box.write(f"ğŸš€ Starting parallel batch: {start_str} â†’ {end_str} (parallel={parallel_degree})")
                
                cmd = [
                    sys.executable, "scripts/batch_run.py",
                    "--start", start_str,
                    "--end", end_str,
                    "--parallel", str(parallel_degree)
                ]
                if skip_exist:
                    cmd.append("--skip")
                
                # è¿è¡Œæ‰¹é‡è„šæœ¬å¹¶å®æ—¶è¯»å–è¾“å‡º
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1
                )
                
                total_days_estimate = (end_d - start_d).days + 1
                completed = 0
                
                for line in process.stdout:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£æè¿›åº¦
                    if line.startswith("[") and "%]" in line:
                        try:
                            pct = int(line.split("%]")[0].replace("[", "").strip())
                            progress_bar.progress(pct / 100)
                            completed += 1
                        except:
                            pass
                        log_box.write(line)
                    elif "===" in line or "æ‰¾åˆ°" in line:
                        log_box.write(f"**{line}**")
                    else:
                        log_box.write(line)
                
                process.wait()
                progress_bar.progress(1.0)
                
                if process.returncode == 0:
                    st.success("âœ… Batch processing complete!")
                    st.balloons()
                else:
                    st.error("âŒ Batch processing failed")


elif page == "SIMULATION":
    swiss_header(T('sim_title'), T('sim_subtitle'))
    
    # 1. Settings
    with st.container():
        st.markdown(f"##### {T('sim_settings')}")
        c1, c2, c3 = st.columns(3)
        init_capital = c1.number_input(T('sim_capital'), value=1000000, step=100000)
        hold_period = c2.number_input(T('sim_period'), value=10, min_value=1)
        
        # Get strategies from logs or summary
        summary_df = get_summary_with_refresh()
        strategies = summary_df['ç­–ç•¥'].unique().tolist() if not summary_df.empty else []
        target_strat = c3.selectbox(T('sim_strategy'), strategies)
        
        # Date Range Selection
        available_dates = get_logs_dates()
        min_date = available_dates[0] if available_dates else date.today()
        max_date = date.today()
        
        c4, c5 = st.columns(2)
        start_date_input = c4.date_input("Start Date", value=min_date, min_value=date(2020, 1, 1), max_value=max_date)
        end_date_input = c5.date_input("End Date", value=max_date, min_value=date(2020, 1, 1), max_value=max_date)

    if st.button(T('sim_run'), type="primary", disabled=not strategies):
        # 2. Simulation Logic
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        status_text.text("Loading logs...")
        
        # A. Load all relevant logs
        logs_dir = Path("logs")
        log_files = sorted(list(logs_dir.glob("*é€‰è‚¡.csv")))
        
        signals = {} # Date -> [Codes]
        
        for f in log_files:
            try:
                date_str = f.stem.replace("é€‰è‚¡", "") # YYYY-MM-DD
                d = datetime.strptime(date_str, "%Y-%m-%d").date()
                
                # Filter by date range
                if d < start_date_input or d > end_date_input:
                    continue
                
                df_log = pd.read_csv(f, dtype=str, encoding='utf-8-sig') # Handle BOM
                
                # Map Chinese columns
                col_map = {
                    'ä»£ç ': 'code',
                    'symbol': 'code',
                    'ts_code': 'code',
                    'ç­–ç•¥': 'strategies',
                    'strategy': 'strategies'
                }
                df_log = df_log.rename(columns=col_map)
                
                # Normalize code
                if 'code' in df_log.columns:
                     # Remove .SZ/.SH if present
                    df_log['code'] = df_log['code'].astype(str).str.split('.').str[0].str.zfill(6)
                
                if 'code' in df_log.columns and 'strategies' in df_log.columns:
                    # Filter for strategy
                    df_strat = df_log[df_log['strategies'].astype(str).str.contains(target_strat, regex=False)]
                    codes = df_strat['code'].tolist()
                    if codes:
                        signals[d] = codes
            except Exception as e:
                pass
        
        if not signals:
            st.warning(T('sim_no_logs'))
        else:
            # Sort dates
            sorted_dates = sorted(signals.keys())
            # Use user selected range (though signals are already filtered, simulation timeline should match)
            start_date = start_date_input
            end_date = end_date_input
            
            status_text.text(f"Simulating range: {start_date} -> {end_date}")

            
            # Load Index for Benchmark (HS300)
            bm_df = pd.DataFrame()
            try:
                bm_df = pd.read_parquet("data_parquet/000300.parquet", columns=['date', 'open', 'close'])
                bm_df['date'] = pd.to_datetime(bm_df['date']).dt.date
                bm_df = bm_df.sort_values('date').set_index('date')
            except:
                pass

            # B. Portfolio Simulation
            # Rules:
            # - T: Generate Signal
            # - T+1: Buy at Open
            # - T+1+HoldDays: Sell at Close
            # - Allocation: Divide capital into (HoldDays + 2) parts to ensure liquidity
            
            cash = init_capital
            positions = {} # Code -> {shares, cost, buy_date}
            total_assets = [] # [{date, value}]
            
            # Create full timeline
            timeline = pd.date_range(start_date, end_date, freq='B').date
            
            # Cache stock data to avoid repeated reads
            stock_cache = {}
            
            def get_stock_price(code, d):
                if code not in stock_cache:
                    p = Path(f"data_parquet/{code}.parquet")
                    if p.exists():
                        try:
                            df = pd.read_parquet(p, columns=['date', 'open', 'close'])
                            df['date'] = pd.to_datetime(df['date']).dt.date
                            stock_cache[code] = df.set_index('date')
                        except:
                            stock_cache[code] = pd.DataFrame()
                    else:
                        stock_cache[code] = pd.DataFrame()
                
                df = stock_cache[code]
                if d in df.index:
                    return df.loc[d]
                return None

            allocation_per_slot = init_capital / (hold_period * 1.5) # Conservative allocation
            
            for i, today in enumerate(timeline):
                progress_bar.progress(min(i / len(timeline), 1.0))
                status_text.text(f"Processing {today}...")
                
                # 1. Update Positions & Check Sell
                current_pos_val = 0
                to_remove = []
                
                for code, pos in positions.items():
                    price_data = get_stock_price(code, today)
                    
                    if price_data is not None:
                        current_price = price_data['close']
                        days_held = (today - pos['buy_date']).days
                        # Wait, trading days diff would be better, but simple days is ok for sim
                        
                        # Sell Logic
                        if days_held >= hold_period:
                            # Sell at Close
                            cash += pos['shares'] * current_price
                            to_remove.append(code)
                        else:
                            current_pos_val += pos['shares'] * current_price
                    else:
                        # No data today (suspended?), keep last value (approx) or 0? 
                        # Use cost as proxy if suspended
                        current_pos_val += pos['shares'] * pos['cost'] # Simplified
                
                for c in to_remove:
                    del positions[c]
                    
                # 2. Check Buy (Signals from T-1? Or T? user said "Next day open")
                # So if signal is on T-1 (yesterday), we buy Today Open
                
                # Get signals from Yesterday (or previous trading day in logs)
                # Since 'signals' dict uses log date (which is usually signal date),
                # We need to find if there was a signal yesterday?
                # Actually, iterate days. If 'today' is T, we check signals from T-1.
                # But timeline is business days.
                
                # Simplified: Loop through past dates in signals that match (today - 1_day)?
                # Or just check if today is a trading day and buy based on "pending signals".
                
                # Let's say: Signal produced on Date X. Execution on Date X+1 (if trading day).
                prev_day = today - timedelta(days=1)
                # Find closest signal date <= prev_day? 
                # Actually, simplest is: Check if (today - 1) in signals?
                # But weekends exist.
                
                # Better approach: Iterate signals. If signal_date == prev_trading_day -> Buy Today.
                # However, we are iterating days.
                
                # Let's check if there are signals with date < today that haven't been processed?
                # No, just check specific previous day is tricky.
                # Let's look up signals for (today - gap).
                # But gap varies.
                
                # Workaround: Check signals for yesterday, day before yesterday... up to 5 days?
                # Only if not bought yet.
                # Actually, strict T+1 means: if Signal on Fri, Buy on Mon.
                
                # Let's try: Check signals[today] -> Queue for purchase Next Day?
                # Yes.
                # But this loop is "Today". So we check "Queue".
                
                pass 
                
            # Re-implement loop with Queue
            cash = init_capital
            positions = {}
            queue = [] # (code, signal_date)
            
            history_equity = []
            
            # Align timeline with signals
            # Merge signals keys into timeline to ensure we cover all signal days
            all_days = sorted(list(set(list(timeline) + list(signals.keys()))))
            start_idx = 0
            # Find start index
            for idx, d in enumerate(all_days):
                if d >= start_date:
                    start_idx = idx
                    break
            
            sim_dates = all_days[start_idx:]
            
            for i, today in enumerate(sim_dates):
                progress_bar.progress(min(i / len(sim_dates), 1.0))
                
                # 1. Process Buy Queue (from previous signal)
                new_queue = []
                for code in queue:
                    # Try buy at Open
                    price_data = get_stock_price(code, today)
                    if price_data is not None:
                        open_price = price_data['open']
                        if open_price > 0 and cash > allocation_per_slot:
                            # Buy
                            shares = int(allocation_per_slot / open_price / 100) * 100
                            if shares > 0:
                                cost = shares * open_price
                                cash -= cost
                                positions[code] = {'shares': shares, 'cost': open_price, 'buy_date': today, 'last_price': open_price}
                    else:
                        # Keep in queue for next day? Or discard?
                        # Usually discard if not actionable immediately to avoid piling up
                        pass
                queue = [] # Clear queue, unprocessed are dropped (strict timing)
                
                # 2. Update Position Values & Check Sell
                pos_val = 0
                to_sell = []
                for code, pos in positions.items():
                    price_data = get_stock_price(code, today)
                    price = pos['last_price']
                    
                    if price_data is not None:
                        price = price_data['close']
                        positions[code]['last_price'] = price
                        
                        # Check exit
                        # Hold period check (Trading days vs Calendar days?)
                        # User said "Hold 10 days". Usually calendar days or N trading bars.
                        # Let's use Calendar days for simplicity.
                        if (today - pos['buy_date']).days >= hold_period:
                            to_sell.append(code)
                    
                    pos_val += pos['shares'] * price
                
                # Process Sells
                for code in to_sell:
                    # Sell at Close price of today
                    p = positions[code]
                    cash += p['shares'] * p['last_price']
                    # Remove from pos_val (it's now cash)
                    pos_val -= p['shares'] * p['last_price']
                    del positions[code]
                
                total_equity = cash + pos_val
                
                # Benchmark return
                bm_ret = 0
                if not bm_df.empty and today in bm_df.index:
                    # Relative to start
                    # Look up start price?
                    pass 
                
                history_equity.append({'date': today, 'equity': total_equity})
                
                # 3. Add Today's Signals to Queue (for tomorrow)
                if today in signals:
                    for code in signals[today]:
                        # Avoid buying if already held
                        if code not in positions:
                            queue.append(code)
            
            # C. Visualisation
            res_df = pd.DataFrame(history_equity)
            if not res_df.empty:
                res_df['date'] = pd.to_datetime(res_df['date'])
                
                # Calculate metrics
                final_equity = res_df.iloc[-1]['equity']
                total_ret_pct = (final_equity - init_capital) / init_capital * 100
                res_df['dd'] = (res_df['equity'].cummax() - res_df['equity']) / res_df['equity'].cummax() * 100
                max_dd = res_df['dd'].max()
                
                st.markdown(f"### {T('sim_result')}")
                m1, m2, m3 = st.columns(3)
                m1.metric(T('sim_final_asset'), f"Â¥{final_equity:,.0f}")
                m2.metric(T('sim_total_ret'), f"{total_ret_pct:+.2f}%")
                m3.metric(T('sim_max_dd'), f"{max_dd:.2f}%", delta_color="inverse")
                
                st.markdown("#### èµ„é‡‘æ›²çº¿ (Net Value)")
                # Add benchmark to chart
                chart_df = res_df[['date', 'equity']].copy()
                chart_df['Strategy'] = (chart_df['equity'] - init_capital) / init_capital * 100
                
                # HS300 Benchmark
                if not bm_df.empty:
                    # Align dates
                    bm_chart = bm_df.loc[res_df.iloc[0]['date'].date():res_df.iloc[-1]['date'].date()]
                    if not bm_chart.empty:
                        base_idx = bm_chart.iloc[0]['close']
                        # Map dates to chart_df
                        # We can merge
                        bm_series = bm_chart['close'].reindex(res_df['date'].dt.date).fillna(method='ffill')
                        chart_df['HS300'] = (bm_series.values - base_idx) / base_idx * 100
                
                fig = px.line(chart_df, x='date', y=['Strategy', 'HS300'] if 'HS300' in chart_df.columns else ['Strategy'])
                fig.update_layout(title="ç´¯è®¡æ”¶ç›Šç‡ (%)", hovermode="x unified")
                st.plotly_chart(fig, use_container_width=True)
                
            else:
                st.error("Simulation produced no data.")
                
        status_text.empty()
        progress_bar.empty()

elif page == "BACKTEST":
    swiss_header(T('bt_title'), T('bt_subtitle'))
    
    st.markdown(f"##### {T('bt_select_logs')}")
    
    # scan logs and results
    data = []
    log_dir = Path("logs")
    res_dir = Path("results")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¥æœŸ
    dates = get_logs_dates()
    
    for d in dates:
        d_str = str(d)
        log_p = log_dir / f"{d_str}é€‰è‚¡.csv"
        res_p = res_dir / f"å›æµ‹ç»“æœ_{d_str}.csv"
        
        data.append({
            "date": d,
            "log": "âœ…" if log_p.exists() else "âŒ",
            "result": "âœ…" if res_p.exists() else "âŒ",
            # internal use
            "date_str": d_str,
            "log_path": str(log_p) if log_p.exists() else None,
            "has_log": log_p.exists()
        })
    
    if not data:
        st.info("No logs found.")
    else:
        df_status = pd.DataFrame(data)
        
        # ä½¿ç”¨ DataEditor è®©ç”¨æˆ·é€‰æ‹©
        # æ·»åŠ ä¸€ä¸ª 'Select' åˆ—
        df_status.insert(0, "select", False)

        # å…¨é€‰æœªå›æµ‹æŒ‰é’®
        # å¿…é¡»ä½¿ç”¨ st.session_state æ¥æ›´æ–° data_editor çš„æ•°æ®
        if "pending_select_updates" not in st.session_state:
            st.session_state["pending_select_updates"] = {}

        if st.button(T('bt_btn_select_pending')):
            # æ‰¾åˆ°æ‰€æœ‰æ²¡æœ‰å›æµ‹ç»“æœï¼ˆresult == 'âŒ'ï¼‰ä¸”æœ‰æ—¥å¿—çš„è¡Œ
            mask = (df_status['result'] == 'âŒ') & (df_status['has_log'])
            # å°†è¿™äº›è¡Œçš„ select è®¾ä¸º True
            # æ³¨æ„: st.data_editor ä¼šé‡æ–°åŠ è½½ df_statusï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´æ¥ä¿®æ”¹ df_status
            df_status.loc[mask, 'select'] = True
            st.success(f"Selected {mask.sum()} pending logs.")

        # é…ç½®åˆ—æ˜¾ç¤º
        edited_df = st.data_editor(
            df_status,
            column_config={
                "select": st.column_config.CheckboxColumn("Run", default=False),
                "date": st.column_config.DateColumn(T('bt_table_date'), disabled=True),
                "log": st.column_config.TextColumn(T('bt_table_log'), disabled=True),
                "result": st.column_config.TextColumn(T('bt_table_result'), disabled=True),
                # Hide internal columns
                "date_str": None,
                "log_path": None,
                "has_log": None
            },
            hide_index=True,
            use_container_width=True,
            height=400,
            key="bt_editor" # ç»™ä¸€ä¸ª key ä»¥ä¾¿å¯èƒ½çš„çŠ¶æ€ç®¡ç†
        )
        
        # æå–é€‰ä¸­çš„è¡Œ
        selected_rows = edited_df[edited_df["select"] == True]
        
        if st.button(T('bt_btn_run'), type="primary", disabled=selected_rows.empty):
            count = len(selected_rows)
            st.info(f"Queueing {count} tasks...")
            
            prog_bar = st.progress(0)
            log_area = st.expander(T('bt_log_preview'), expanded=True)
            
            for i, row in enumerate(selected_rows.itertuples()):
                d_str = row.date_str
                log_path = row.log_path
                
                if not row.has_log:
                    log_area.warning(f"Skipping {d_str}: No log file.")
                    continue
                
                log_area.write(f"â–¶ï¸ **Backtesting {d_str}...**")
                
                try:
                    cmd_bt = [sys.executable, "scripts/backtest.py", log_path]
                    ret = subprocess.run(cmd_bt, capture_output=True, text=True)
                    
                    if ret.returncode == 0:
                        log_area.code(ret.stdout)
                        log_area.success(f"âœ… {d_str} Done")
                    else:
                        log_area.error(f"âŒ {d_str} Failed")
                        log_area.code(ret.stderr)
                except Exception as e:
                    log_area.error(f"Error: {e}")
                
                prog_bar.progress((i + 1) / count)
                
            st.success(T('success_finish'))
            # æ›´æ–°åˆ†æç»“æœ
            subprocess.run([sys.executable, "scripts/analyze_results.py"], check=False)


elif page == "SETTINGS":
    swiss_header(T('set_title'), T('set_subtitle'))
    
    st.markdown(f"##### {T('set_token_config')}")
    
    current_token = os.environ.get("TUSHARE_TOKEN", "")
    new_token = st.text_input(T('input_token'), value=current_token, type="password")
    
    if st.button(T('btn_save_token')):
        save_token(new_token)
        st.success(T('msg_token_saved'))
        
    st.markdown("---")
    st.markdown(f"##### {T('set_update_data')}")
    st.markdown(T('set_update_desc'))
    
    c1, c2 = st.columns(2)
    with c1:
        start_d = st.date_input(T('input_start'), value=date(2026, 1, 1))
    with c2:
        end_d = st.date_input(T('input_end'), value=date.today())
        
    st.markdown(f"**{T('set_workers')}**")
    workers_opt = st.radio(
        T('set_workers_help'),
        [1, 6],
        format_func=lambda x: T('set_workers_low') if x == 1 else T('set_workers_high')
    )
        
    if st.button(T('btn_fetch')):
        st.info(T('msg_fetch_start'))
        cmd_fetch = [
            sys.executable, "scripts/fetch_kline.py", 
            "--start", str(start_d), 
            "--end", str(end_d),
            "--workers", str(workers_opt),
            "--use-token" 
        ]
        
        try:
            subprocess.Popen(cmd_fetch) 
            st.success(T('msg_bg_start'))
        except Exception as e:
            st.error(T('err_start_fail', e=e))
